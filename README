## Diversity-Preserving Crossover Exploitation

**Authors:** *Tom Offermann*, *Prof. Dr. Johannes Lengler*

**Institution:** ETH Zürich, Switzerland

This repository includes all the code for running the simulations in the following paper: *Diversity-Preserving Crossover Exploitation*, "TODO: Add link to paper". It may also be useful for simulating & plotting the performance of other optimization algorithms. This template is very minimal but offers simple scheduling and plotting for the performance of different optimization algorithms. These implementationy focus on Genetic Algorithms (GAs) for optimizing pseudo-boolean functions $f: \{0,1\}^n \mapsto \mathbb R$.

### How to get started
1. Clone this repository & make sure python (atleast ```3.9```, check with ```python --v```) is correctly installed
2. Make sure all necessary packages are installed (if not use ```pip install ...```)

### General Structure
* The ```./data``` folder is used as a database for all the runs and can also be seen as a cache for runs that were already run.

* Modify the ```run.py``` script to start new simulations (wont rerun any simulations previously started with the same params, delete the corresponding file in ```./data``` to force a rerun)

* Modify the ```plot.py``` script to create plots from the simulations. The plots will appear in the ```./plots``` folder. 

### Implementation Details & Provided Functionality

Implementations for the DEGA variants and other GAs are given in ```./src/algorithms```. The code leaves a lot of room for natural optimizations, if you have time to optimize some of the implementations, feel free to submit a pull request :)

Notice that we are mentioning three different implementations for a DEGA in "TODO: Add link to paper":
1. **DEGA** - Simplest implementation (used in the proofs) and defined in Section 2 of the paper
2. **DEGA_A** - More Robust Version, discussed in Section 5.1 (flowchart)
3. **DEGA_B** - Illustrative version utilizing algorithmic idea from [1] (More tailored towards *LeadingOnes*)

Other algorithms include:
* **$(2+1)$-GA**
* **$(1+1)$-GA**
* **$(1+(\lambda, \lambda))$-GA**
* **UMDA**

These algorithms can be tested on the following benchmarks (subset of the PBO suite from the IOHAnalyzer [2]):
* *LeadingOnes*
* *OneMax*
* *JUMP*
* *LFWH* (Linear Function, Harmonic Weights)
* *MIVS* (Maximum Independent Vertex Set)


[1] Benjamin Doerr, Daniel Johannsen, Timo Kötzing, Per Kristian Lehre, Markus
Wagner, and Carola Winzen. 2011. Faster black-box algorithms through higher
arity operators. In Foundations of Genetic Algorithms (FOGA 2011). ACM, New
York, 163–172.

[2] Hao Wang, Diederick Vermetten, Furong Ye, Carola Doerr, and Thomas Bäck.
2022. IOHanalyzer: Detailed Performance Analyses for Iterative Optimization
Heuristics. ACM Transactions on Evolutionary Learning and Optimization 2, 1
(apr 2022), 29 pages. https://doi.org/10.1145/3510426